{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20247e09-0abf-4ca9-ba29-15fc61ac575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define the base path\n",
    "base_path = '300WLPA_2d'\n",
    "helen_path = os.path.join(base_path, 'HELEN')\n",
    "afw_path = os.path.join(base_path, 'AFW')\n",
    "lfpw_path = os.path.join(base_path, 'LFPW')\n",
    "\n",
    "# Define train, val, test directories\n",
    "train_dir = os.path.join(base_path, 'train')\n",
    "val_dir = os.path.join(base_path, 'val')\n",
    "test_dir = os.path.join(base_path, 'test')\n",
    "\n",
    "# Create the directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Helper function to split data\n",
    "def split_data(source_dir, train_dir, val_dir, test_dir, train_ratio=0.8, val_ratio=0.1):\n",
    "    subfolders = [f.path for f in os.scandir(source_dir) if f.is_dir()]\n",
    "    for subfolder in subfolders:\n",
    "        files = [f for f in os.listdir(subfolder) if os.path.isfile(os.path.join(subfolder, f))]\n",
    "        random.shuffle(files)\n",
    "        train_split = int(len(files) * train_ratio)\n",
    "        val_split = int(len(files) * (train_ratio + val_ratio))\n",
    "        \n",
    "        train_files = files[:train_split]\n",
    "        val_files = files[train_split:val_split]\n",
    "        test_files = files[val_split:]\n",
    "\n",
    "        for file in train_files:\n",
    "            shutil.copy(os.path.join(subfolder, file), train_dir)\n",
    "        \n",
    "        for file in val_files:\n",
    "            shutil.copy(os.path.join(subfolder, file), val_dir)\n",
    "\n",
    "        for file in test_files:\n",
    "            shutil.copy(os.path.join(subfolder, file), test_dir)\n",
    "\n",
    "# Split the data\n",
    "split_data(helen_path, train_dir, val_dir, test_dir)\n",
    "split_data(afw_path, train_dir, val_dir, test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa3bf69-a8e0-4d58-b4c8-a98b384ef789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch import nn, optim\n",
    "\n",
    "# Constants for class ranges\n",
    "NUM_CLASSES_PITCH_ROLL = 66\n",
    "NUM_CLASSES_YAW = 120\n",
    "TRAIN_SIZE = 10000\n",
    "VAL_SIZE = 1000\n",
    "TEST_SIZE = 1000\n",
    "\n",
    "# Helper function to map continuous values to class indices\n",
    "def continuous_to_class(value, num_classes, angle_range):\n",
    "    # Convert radian to degree\n",
    "    value = math.degrees(value)\n",
    "    # Normalize to [0, angle_range]\n",
    "    value = (value + angle_range / 2) % angle_range\n",
    "    # Calculate bin\n",
    "    return min(num_classes - 1, max(0, int(value // (angle_range / num_classes))))\n",
    "\n",
    "# Function to convert class indices to one-hot tensors\n",
    "def class_to_onehot(class_indices, num_classes):\n",
    "    if class_indices.dim() == 0:\n",
    "        class_indices = class_indices.unsqueeze(0)\n",
    "    batch_size = class_indices.size(0)\n",
    "    onehot_tensor = torch.zeros(batch_size, num_classes)\n",
    "    onehot_tensor.scatter_(1, class_indices.view(-1, 1).long(), 1)\n",
    "    return onehot_tensor\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Extract yaw, pitch, and roll from the filename\n",
    "        filename = os.path.basename(image_path)\n",
    "        parts = filename.split('_')\n",
    "        # print(parts)\n",
    "        pitch_value = float(parts[-3])\n",
    "        roll_value = float(parts[-1].split('.jpg')[0])\n",
    "        yaw_value = float(parts[-2])\n",
    "        # print(\"Filename\")\n",
    "        # print(filename)\n",
    "        # print()\n",
    "        # print(\"Yaw Pitch Roll in Radians\")\n",
    "        # print(yaw_value,\" \", pitch_value,\" \", roll_value)\n",
    "\n",
    "        # Convert values to class indices\n",
    "        yaw = continuous_to_class(yaw_value, NUM_CLASSES_YAW, 360)\n",
    "        pitch = continuous_to_class(pitch_value, NUM_CLASSES_PITCH_ROLL, 198)\n",
    "        roll = continuous_to_class(roll_value, NUM_CLASSES_PITCH_ROLL, 198)\n",
    "        # print(\"Yaw Pitch Roll in Degrees\")\n",
    "        # print(yaw,\" \", pitch,\" \", roll)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert class indices to one-hot tensors\n",
    "        yaw_onehot = class_to_onehot(torch.tensor(yaw), NUM_CLASSES_YAW)\n",
    "        pitch_onehot = class_to_onehot(torch.tensor(pitch), NUM_CLASSES_PITCH_ROLL)\n",
    "        roll_onehot = class_to_onehot(torch.tensor(roll), NUM_CLASSES_PITCH_ROLL)\n",
    "\n",
    "        # print(\"Yaw Pitch Roll Onehot Shapes\")\n",
    "        # print(yaw_onehot.shape,\" \",pitch_onehot.shape,\" \",roll_onehot.shape)\n",
    "        \n",
    "        labels = {\n",
    "            'yaw': yaw_onehot,\n",
    "            'pitch': pitch_onehot,\n",
    "            'roll': roll_onehot\n",
    "        }\n",
    "        \n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab9ab85-5698-4c67-973b-6a8a1d045705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 10000\n",
      "Validation dataset size: 1000\n",
      "Test dataset size: 1000\n",
      "\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 1, 120])\n",
      "torch.Size([32, 1, 66])\n",
      "torch.Size([32, 1, 66])\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 82, Pitch Label: 21, Roll Label: 29\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5BFFAE50>, Yaw Label: 56, Pitch Label: 21, Roll Label: 34\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 86, Pitch Label: 23, Roll Label: 32\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 78, Pitch Label: 26, Roll Label: 33\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 85, Pitch Label: 34, Roll Label: 28\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 74, Pitch Label: 35, Roll Label: 30\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 46, Pitch Label: 38, Roll Label: 33\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 31, Pitch Label: 43, Roll Label: 39\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 88, Pitch Label: 21, Roll Label: 26\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 35, Pitch Label: 23, Roll Label: 30\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 59, Pitch Label: 41, Roll Label: 33\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 65, Pitch Label: 20, Roll Label: 38\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 52, Pitch Label: 19, Roll Label: 32\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 64, Pitch Label: 18, Roll Label: 32\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 51, Pitch Label: 26, Roll Label: 33\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 79, Pitch Label: 24, Roll Label: 30\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 72, Pitch Label: 22, Roll Label: 35\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 70, Pitch Label: 22, Roll Label: 35\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 60, Pitch Label: 27, Roll Label: 32\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 37, Pitch Label: 29, Roll Label: 32\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 51, Pitch Label: 43, Roll Label: 29\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C71D0>, Yaw Label: 89, Pitch Label: 41, Roll Label: 33\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 39, Pitch Label: 25, Roll Label: 43\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7210>, Yaw Label: 40, Pitch Label: 27, Roll Label: 38\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7190>, Yaw Label: 69, Pitch Label: 40, Roll Label: 36\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7210>, Yaw Label: 87, Pitch Label: 42, Roll Label: 35\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7110>, Yaw Label: 62, Pitch Label: 25, Roll Label: 33\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7250>, Yaw Label: 63, Pitch Label: 41, Roll Label: 32\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7290>, Yaw Label: 32, Pitch Label: 38, Roll Label: 35\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C72D0>, Yaw Label: 38, Pitch Label: 26, Roll Label: 32\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7310>, Yaw Label: 49, Pitch Label: 39, Roll Label: 28\n",
      "Image: <PIL.Image.Image image mode=RGB size=224x224 at 0x25F5F3C7350>, Yaw Label: 73, Pitch Label: 26, Roll Label: 27\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Constants for class ranges and dataset sizes\n",
    "NUM_CLASSES_PITCH_ROLL = 66\n",
    "NUM_CLASSES_YAW = 120\n",
    "TRAIN_SIZE = 10000\n",
    "VAL_SIZE = 1000\n",
    "TEST_SIZE = 1000\n",
    "\n",
    "# Define the base path\n",
    "base_path = '300WLPA_2d'\n",
    "train_dir = os.path.join(base_path, 'train')\n",
    "val_dir = os.path.join(base_path, 'val')\n",
    "test_dir = os.path.join(base_path, 'test')\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create the full datasets\n",
    "full_train_dataset = CustomImageDataset(train_dir, transform=transform)\n",
    "full_val_dataset = CustomImageDataset(val_dir, transform=transform)\n",
    "full_test_dataset = CustomImageDataset(test_dir, transform=transform)\n",
    "\n",
    "# Generate indices for the datasets\n",
    "train_indices = list(range(len(full_train_dataset)))\n",
    "val_indices = list(range(len(full_val_dataset)))\n",
    "test_indices = list(range(len(full_test_dataset)))\n",
    "\n",
    "# Shuffle indices\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(train_indices)\n",
    "random.shuffle(val_indices)\n",
    "random.shuffle(test_indices)\n",
    "\n",
    "# Create subsets\n",
    "train_dataset = Subset(full_train_dataset, train_indices[:TRAIN_SIZE])\n",
    "val_dataset = Subset(full_val_dataset, val_indices[:VAL_SIZE])\n",
    "test_dataset = Subset(full_test_dataset, test_indices[:TEST_SIZE])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print sizes of datasets\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print()\n",
    "\n",
    "# Example of iterating through the train_loader\n",
    "for images, labels in train_loader:\n",
    "    # images: tensor of shape (batch_size, 3, 224, 224)\n",
    "    # labels['yaw']: tensor of shape (batch_size, NUM_CLASSES_YAW)\n",
    "    # labels['pitch']: tensor of shape (batch_size, NUM_CLASSES_PITCH_ROLL)\n",
    "    # labels['roll']: tensor of shape (batch_size, NUM_CLASSES_PITCH_ROLL)\n",
    "    print(images.shape)\n",
    "    print(labels['yaw'].shape)\n",
    "    print(labels['pitch'].shape)\n",
    "    print(labels['roll'].shape)\n",
    "    \n",
    "    # Example: iterating through the batch\n",
    "    for i in range(images.size(0)):\n",
    "        image = transforms.ToPILImage()(images[i])\n",
    "        \n",
    "        # Get the indices of the maximum value in each one-hot tensor (argmax)\n",
    "        yaw_label = torch.argmax(labels['yaw'][i]).item()\n",
    "        pitch_label = torch.argmax(labels['pitch'][i]).item()\n",
    "        roll_label = torch.argmax(labels['roll'][i]).item()\n",
    "        \n",
    "        # Print or use these labels as needed\n",
    "        print(f\"Image: {image}, Yaw Label: {yaw_label}, Pitch Label: {pitch_label}, Roll Label: {roll_label}\")\n",
    "    \n",
    "    break  # Break after the first batch for demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c9f5e58e-8ad0-4fe9-88d7-08e3ec70fad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AFW', '2043831280', '2', '7', '-0.613', '-1.170', '-0.057.jpg']\n",
      "-0.057\n"
     ]
    }
   ],
   "source": [
    "x = \"AFW_2043831280_2_7_-0.613_-1.170_-0.057.jpg\"\n",
    "parts = x.split('_')\n",
    "print(parts)\n",
    "\n",
    "print(parts[-1].split('.jpg')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0192696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seif_Eldin_Sameh\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seif_Eldin_Sameh\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained MobileNetV3 model\n",
    "model_MNV3 = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "# Define constants\n",
    "HIDDEN_FEATURES = 1024\n",
    "OUTPUT_FEATURES = 1280\n",
    "NUM_CLASSES = 66  # Number of classes for each roll and pitch\n",
    "NUM_CLASSES_YAW = 120\n",
    "\n",
    "# Define the custom classifier with three heads\n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(CustomHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, HIDDEN_FEATURES)\n",
    "        self.fc2 = nn.Linear(HIDDEN_FEATURES, OUTPUT_FEATURES)\n",
    "        self.fc_yaw = nn.Linear(OUTPUT_FEATURES, NUM_CLASSES_YAW)\n",
    "        self.fc_pitch = nn.Linear(OUTPUT_FEATURES, NUM_CLASSES)\n",
    "        self.fc_roll = nn.Linear(OUTPUT_FEATURES, NUM_CLASSES)\n",
    "        self.hardswish = nn.Hardswish()\n",
    "        self.dropout = nn.Dropout(p=0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hardswish(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.hardswish(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        yaw = self.fc_yaw(x)\n",
    "        pitch = self.fc_pitch(x)\n",
    "        roll = self.fc_roll(x)\n",
    "        return yaw, pitch, roll\n",
    "\n",
    "# Modify the MobileNetV3 model to use the custom classifier\n",
    "class ModifiedMobileNetV3(nn.Module):\n",
    "    def __init__(self, num_classes_yaw, num_classes_pitch, num_classes_roll):\n",
    "        super(ModifiedMobileNetV3, self).__init__()\n",
    "        self.backbone = models.mobilenet_v3_small(pretrained=True).features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = models.mobilenet_v3_small(pretrained=True).classifier[0].in_features\n",
    "        self.classifier = CustomHead(in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        yaw, pitch, roll = self.classifier(x)\n",
    "        return yaw, pitch, roll\n",
    "\n",
    "# Instantiate the modified model\n",
    "model_MNV3 = ModifiedMobileNetV3(NUM_CLASSES_YAW, NUM_CLASSES, NUM_CLASSES)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model_MNV3 = model_MNV3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc593e4c-62be-43e3-88ca-0ed2983698bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, alpha, beta):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.regression_loss = nn.MSELoss()\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, yaw_pred, pitch_pred, roll_pred, yaw_true, pitch_true, roll_true):\n",
    "        yaw_true = torch.squeeze(yaw_true, dim=1)\n",
    "        pitch_true = torch.squeeze(pitch_true, dim=1)\n",
    "        roll_true = torch.squeeze(roll_true, dim=1)\n",
    "        \n",
    "        # Regression losses\n",
    "        reg_loss_yaw = self.regression_loss(yaw_pred.float(), yaw_true.float())\n",
    "        reg_loss_pitch = self.regression_loss(pitch_pred.float(), pitch_true.float())\n",
    "        reg_loss_roll = self.regression_loss(roll_pred.float(), roll_true.float())\n",
    "        \n",
    "        # Convert true values to class indices\n",
    "        # yaw_true = torch.argmax(yaw_true, dim=1)\n",
    "        # pitch_true = torch.argmax(pitch_true, dim=1)\n",
    "        # roll_true = torch.argmax(roll_true, dim=1)\n",
    "        \n",
    "        # Convert one-hot encoded predictions to logits\n",
    "        yaw_pred_logits = F.softmax(yaw_pred, dim=1)\n",
    "        pitch_pred_logits = F.softmax(pitch_pred, dim=1)\n",
    "        roll_pred_logits = F.softmax(roll_pred, dim=1)\n",
    "\n",
    "        # print(\"PRED SHAPE\")\n",
    "        # print(yaw_pred_logits.shape)\n",
    "        # print(\"TRUE SHAPE\")\n",
    "        # print(yaw_true.shape)\n",
    "        # Classification losses (using CrossEntropyLoss)\n",
    "        cls_loss_yaw = F.cross_entropy(yaw_pred_logits, yaw_true)\n",
    "        cls_loss_pitch = F.cross_entropy(pitch_pred_logits, pitch_true)\n",
    "        cls_loss_roll = F.cross_entropy(roll_pred_logits, roll_true)\n",
    "\n",
    "        # Combined loss\n",
    "        reg_loss = (reg_loss_yaw*3 + reg_loss_pitch*2 + reg_loss_roll) / 3\n",
    "        cls_loss = (cls_loss_yaw*3 + cls_loss_pitch*2 + cls_loss_roll) / 3\n",
    "\n",
    "        loss = self.alpha * reg_loss + self.beta * cls_loss\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6204c4-3913-4aa1-8247-b5f0818fb220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the custom loss with appropriate weights\n",
    "criterion = CustomLoss(alpha=1.0, beta=2.0)  # Adjust alpha and beta as needed\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(model_MNV3.classifier.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b747af6-1ae0-45ba-a878-c4b34d798b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, current_loss):\n",
    "        if current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c5854a-bad3-4eb7-8fe2-945ede677d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "early_stopper = EarlyStopper(patience=20, min_delta=0.5)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10, early_stopper=None, save_path='best_model.pth'):\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    training_accuracies = []  # Placeholder for accuracies, adjust as needed\n",
    "    validation_accuracies = []  # Placeholder for accuracies, adjust as needed\n",
    "    best_val_loss = float('inf')  # Initialize with infinity\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "        print('-' * 50)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        processed_samples_train = 0\n",
    "        total_samples_train = len(train_loader.dataset)\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            yaw_true = labels['yaw'].to(device)\n",
    "            pitch_true = labels['pitch'].to(device)\n",
    "            roll_true = labels['roll'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            yaw_pred, pitch_pred, roll_pred = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(yaw_pred, pitch_pred, roll_pred, yaw_true, pitch_true, roll_true)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            processed_samples_train += inputs.size(0)\n",
    "\n",
    "            # Print progress\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Training Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        epoch_loss = running_loss / total_samples_train\n",
    "        training_losses.append(epoch_loss)\n",
    "        print(f'Training Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct_yaw = 0\n",
    "        correct_pitch = 0\n",
    "        correct_roll = 0\n",
    "        total = 0\n",
    "        processed_samples_val = 0\n",
    "        total_samples_val = len(val_loader.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                yaw_true = labels['yaw'].to(device)\n",
    "                pitch_true = labels['pitch'].to(device)\n",
    "                roll_true = labels['roll'].to(device)\n",
    "        \n",
    "                yaw_pred, pitch_pred, roll_pred = model(inputs)\n",
    "        \n",
    "                # Create one-hot encoded tensors on GPU\n",
    "                predicted_yaw = torch.eye(yaw_pred.shape[1], device=device)[torch.argmax(yaw_pred, dim=1)]\n",
    "                predicted_pitch = torch.eye(pitch_pred.shape[1], device=device)[torch.argmax(pitch_pred, dim=1)]\n",
    "                predicted_roll = torch.eye(roll_pred.shape[1], device=device)[torch.argmax(roll_pred, dim=1)]\n",
    "        \n",
    "                # Compute loss (assuming criterion accepts one-hot encoded targets)\n",
    "                loss = criterion(yaw_pred, pitch_pred, roll_pred, predicted_yaw, predicted_pitch, predicted_roll)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "                total += inputs.size(0)  # Update total number of samples processed\n",
    "\n",
    "                # Print progress\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f'Validation Batch [{batch_idx + 1}/{len(val_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            val_loss = val_running_loss / total_samples_val\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            # Save model with the lowest validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f'New best model saved with validation loss: {val_loss:.4f}')\n",
    "\n",
    "            # Calculate accuracies\n",
    "            val_accuracy_yaw = 100 * correct_yaw / total_samples_val\n",
    "            val_accuracy_pitch = 100 * correct_pitch / total_samples_val\n",
    "            val_accuracy_roll = 100 * correct_roll / total_samples_val\n",
    "\n",
    "            validation_accuracies.append((val_accuracy_yaw + val_accuracy_pitch + val_accuracy_roll) / 3)\n",
    "\n",
    "            print(f'Validation Loss: {val_loss:.4f}, '\n",
    "                  f'Yaw Accuracy: {val_accuracy_yaw:.2f}%, '\n",
    "                  f'Pitch Accuracy: {val_accuracy_pitch:.2f}%, '\n",
    "                  f'Roll Accuracy: {val_accuracy_roll:.2f}%')\n",
    "\n",
    "        # Check early stopping condition\n",
    "        if early_stopper is not None and early_stopper.early_stop(val_loss):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print('Training complete')\n",
    "\n",
    "    return model, training_losses, validation_losses, training_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b9567d74-7b07-45d7-9503-ab11c3214ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.3997\n",
      "Training Batch [20/313], Loss: 4.4001\n",
      "Training Batch [30/313], Loss: 4.4002\n",
      "Training Batch [40/313], Loss: 4.3998\n",
      "Training Batch [50/313], Loss: 4.3998\n",
      "Training Batch [60/313], Loss: 4.3999\n",
      "Training Batch [70/313], Loss: 4.3991\n",
      "Training Batch [80/313], Loss: 4.3998\n",
      "Training Batch [90/313], Loss: 4.3996\n",
      "Training Batch [100/313], Loss: 4.3996\n",
      "Training Batch [110/313], Loss: 4.4000\n",
      "Training Batch [120/313], Loss: 4.3999\n",
      "Training Batch [130/313], Loss: 4.4000\n",
      "Training Batch [140/313], Loss: 4.3998\n",
      "Training Batch [150/313], Loss: 4.3998\n",
      "Training Batch [160/313], Loss: 4.3997\n",
      "Training Batch [170/313], Loss: 4.3997\n",
      "Training Batch [180/313], Loss: 4.4002\n",
      "Training Batch [190/313], Loss: 4.3997\n",
      "Training Batch [200/313], Loss: 4.4002\n",
      "Training Batch [210/313], Loss: 4.3999\n",
      "Training Batch [220/313], Loss: 4.3999\n",
      "Training Batch [230/313], Loss: 4.4000\n",
      "Training Batch [240/313], Loss: 4.4001\n",
      "Training Batch [250/313], Loss: 4.3995\n",
      "Training Batch [260/313], Loss: 4.4000\n",
      "Training Batch [270/313], Loss: 4.3998\n",
      "Training Batch [280/313], Loss: 4.4005\n",
      "Training Batch [290/313], Loss: 4.4003\n",
      "Training Batch [300/313], Loss: 4.3998\n",
      "Training Batch [310/313], Loss: 4.4000\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3974\n",
      "Validation Batch [20/32], Loss: 4.3974\n",
      "Validation Batch [30/32], Loss: 4.3975\n",
      "New best model saved with validation loss: 4.3974\n",
      "Validation Loss: 4.3974, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [2/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.3997\n",
      "Training Batch [20/313], Loss: 4.4005\n",
      "Training Batch [30/313], Loss: 4.3998\n",
      "Training Batch [40/313], Loss: 4.3999\n",
      "Training Batch [50/313], Loss: 4.3997\n",
      "Training Batch [60/313], Loss: 4.3997\n",
      "Training Batch [70/313], Loss: 4.3997\n",
      "Training Batch [80/313], Loss: 4.4000\n",
      "Training Batch [90/313], Loss: 4.3996\n",
      "Training Batch [100/313], Loss: 4.3997\n",
      "Training Batch [110/313], Loss: 4.4002\n",
      "Training Batch [120/313], Loss: 4.3997\n",
      "Training Batch [130/313], Loss: 4.3999\n",
      "Training Batch [140/313], Loss: 4.4002\n",
      "Training Batch [150/313], Loss: 4.3999\n",
      "Training Batch [160/313], Loss: 4.4000\n",
      "Training Batch [170/313], Loss: 4.3995\n",
      "Training Batch [180/313], Loss: 4.4002\n",
      "Training Batch [190/313], Loss: 4.4001\n",
      "Training Batch [200/313], Loss: 4.4000\n",
      "Training Batch [210/313], Loss: 4.3999\n",
      "Training Batch [220/313], Loss: 4.4002\n",
      "Training Batch [230/313], Loss: 4.4001\n",
      "Training Batch [240/313], Loss: 4.4005\n",
      "Training Batch [250/313], Loss: 4.4000\n",
      "Training Batch [260/313], Loss: 4.4000\n",
      "Training Batch [270/313], Loss: 4.3999\n",
      "Training Batch [280/313], Loss: 4.3999\n",
      "Training Batch [290/313], Loss: 4.3997\n",
      "Training Batch [300/313], Loss: 4.3996\n",
      "Training Batch [310/313], Loss: 4.4003\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3974\n",
      "Validation Batch [20/32], Loss: 4.3975\n",
      "Validation Batch [30/32], Loss: 4.3976\n",
      "Validation Loss: 4.3975, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [3/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.3998\n",
      "Training Batch [20/313], Loss: 4.3994\n",
      "Training Batch [30/313], Loss: 4.3994\n",
      "Training Batch [40/313], Loss: 4.4002\n",
      "Training Batch [50/313], Loss: 4.3994\n",
      "Training Batch [60/313], Loss: 4.4000\n",
      "Training Batch [70/313], Loss: 4.4002\n",
      "Training Batch [80/313], Loss: 4.3999\n",
      "Training Batch [90/313], Loss: 4.3999\n",
      "Training Batch [100/313], Loss: 4.3996\n",
      "Training Batch [110/313], Loss: 4.3999\n",
      "Training Batch [120/313], Loss: 4.3994\n",
      "Training Batch [130/313], Loss: 4.4002\n",
      "Training Batch [140/313], Loss: 4.3997\n",
      "Training Batch [150/313], Loss: 4.4001\n",
      "Training Batch [160/313], Loss: 4.3999\n",
      "Training Batch [170/313], Loss: 4.4000\n",
      "Training Batch [180/313], Loss: 4.3996\n",
      "Training Batch [190/313], Loss: 4.3999\n",
      "Training Batch [200/313], Loss: 4.3999\n",
      "Training Batch [210/313], Loss: 4.3996\n",
      "Training Batch [220/313], Loss: 4.3996\n",
      "Training Batch [230/313], Loss: 4.4001\n",
      "Training Batch [240/313], Loss: 4.3998\n",
      "Training Batch [250/313], Loss: 4.3998\n",
      "Training Batch [260/313], Loss: 4.3999\n",
      "Training Batch [270/313], Loss: 4.3997\n",
      "Training Batch [280/313], Loss: 4.3998\n",
      "Training Batch [290/313], Loss: 4.4003\n",
      "Training Batch [300/313], Loss: 4.3998\n",
      "Training Batch [310/313], Loss: 4.3998\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3971\n",
      "Validation Batch [20/32], Loss: 4.3971\n",
      "Validation Batch [30/32], Loss: 4.3973\n",
      "New best model saved with validation loss: 4.3972\n",
      "Validation Loss: 4.3972, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [4/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.3995\n",
      "Training Batch [20/313], Loss: 4.4002\n",
      "Training Batch [30/313], Loss: 4.4000\n",
      "Training Batch [40/313], Loss: 4.3999\n",
      "Training Batch [50/313], Loss: 4.4004\n",
      "Training Batch [60/313], Loss: 4.3997\n",
      "Training Batch [70/313], Loss: 4.3996\n",
      "Training Batch [80/313], Loss: 4.4002\n",
      "Training Batch [90/313], Loss: 4.4002\n",
      "Training Batch [100/313], Loss: 4.3998\n",
      "Training Batch [110/313], Loss: 4.3998\n",
      "Training Batch [120/313], Loss: 4.3997\n",
      "Training Batch [130/313], Loss: 4.3995\n",
      "Training Batch [140/313], Loss: 4.3995\n",
      "Training Batch [150/313], Loss: 4.3998\n",
      "Training Batch [160/313], Loss: 4.3999\n",
      "Training Batch [170/313], Loss: 4.3994\n",
      "Training Batch [180/313], Loss: 4.3997\n",
      "Training Batch [190/313], Loss: 4.3997\n",
      "Training Batch [200/313], Loss: 4.4005\n",
      "Training Batch [210/313], Loss: 4.3999\n",
      "Training Batch [220/313], Loss: 4.3998\n",
      "Training Batch [230/313], Loss: 4.3998\n",
      "Training Batch [240/313], Loss: 4.4000\n",
      "Training Batch [250/313], Loss: 4.3997\n",
      "Training Batch [260/313], Loss: 4.3999\n",
      "Training Batch [270/313], Loss: 4.3999\n",
      "Training Batch [280/313], Loss: 4.4001\n",
      "Training Batch [290/313], Loss: 4.4000\n",
      "Training Batch [300/313], Loss: 4.3996\n",
      "Training Batch [310/313], Loss: 4.4005\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3974\n",
      "Validation Batch [20/32], Loss: 4.3975\n",
      "Validation Batch [30/32], Loss: 4.3976\n",
      "Validation Loss: 4.3975, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [5/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.3994\n",
      "Training Batch [20/313], Loss: 4.3997\n",
      "Training Batch [30/313], Loss: 4.3996\n",
      "Training Batch [40/313], Loss: 4.3999\n",
      "Training Batch [50/313], Loss: 4.4002\n",
      "Training Batch [60/313], Loss: 4.3999\n",
      "Training Batch [70/313], Loss: 4.3995\n",
      "Training Batch [80/313], Loss: 4.4000\n",
      "Training Batch [90/313], Loss: 4.3994\n",
      "Training Batch [100/313], Loss: 4.3997\n",
      "Training Batch [110/313], Loss: 4.4003\n",
      "Training Batch [120/313], Loss: 4.3997\n",
      "Training Batch [130/313], Loss: 4.3994\n",
      "Training Batch [140/313], Loss: 4.3998\n",
      "Training Batch [150/313], Loss: 4.3996\n",
      "Training Batch [160/313], Loss: 4.3998\n",
      "Training Batch [170/313], Loss: 4.3997\n",
      "Training Batch [180/313], Loss: 4.4007\n",
      "Training Batch [190/313], Loss: 4.3998\n",
      "Training Batch [200/313], Loss: 4.4001\n",
      "Training Batch [210/313], Loss: 4.3996\n",
      "Training Batch [220/313], Loss: 4.4001\n",
      "Training Batch [230/313], Loss: 4.4000\n",
      "Training Batch [240/313], Loss: 4.3995\n",
      "Training Batch [250/313], Loss: 4.3999\n",
      "Training Batch [260/313], Loss: 4.4000\n",
      "Training Batch [270/313], Loss: 4.3994\n",
      "Training Batch [280/313], Loss: 4.4002\n",
      "Training Batch [290/313], Loss: 4.3996\n",
      "Training Batch [300/313], Loss: 4.3995\n",
      "Training Batch [310/313], Loss: 4.4000\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3976\n",
      "Validation Batch [20/32], Loss: 4.3976\n",
      "Validation Batch [30/32], Loss: 4.3976\n",
      "Validation Loss: 4.3976, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [6/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.3997\n",
      "Training Batch [20/313], Loss: 4.4002\n",
      "Training Batch [30/313], Loss: 4.3999\n",
      "Training Batch [40/313], Loss: 4.3994\n",
      "Training Batch [50/313], Loss: 4.3998\n",
      "Training Batch [60/313], Loss: 4.3997\n",
      "Training Batch [70/313], Loss: 4.3998\n",
      "Training Batch [80/313], Loss: 4.3995\n",
      "Training Batch [90/313], Loss: 4.3996\n",
      "Training Batch [100/313], Loss: 4.3996\n",
      "Training Batch [110/313], Loss: 4.3998\n",
      "Training Batch [120/313], Loss: 4.3998\n",
      "Training Batch [130/313], Loss: 4.4001\n",
      "Training Batch [140/313], Loss: 4.4004\n",
      "Training Batch [150/313], Loss: 4.4002\n",
      "Training Batch [160/313], Loss: 4.3999\n",
      "Training Batch [170/313], Loss: 4.3996\n",
      "Training Batch [180/313], Loss: 4.3999\n",
      "Training Batch [190/313], Loss: 4.4001\n",
      "Training Batch [200/313], Loss: 4.3998\n",
      "Training Batch [210/313], Loss: 4.3999\n",
      "Training Batch [220/313], Loss: 4.3999\n",
      "Training Batch [230/313], Loss: 4.3996\n",
      "Training Batch [240/313], Loss: 4.4002\n",
      "Training Batch [250/313], Loss: 4.3995\n",
      "Training Batch [260/313], Loss: 4.3999\n",
      "Training Batch [270/313], Loss: 4.3996\n",
      "Training Batch [280/313], Loss: 4.3995\n",
      "Training Batch [290/313], Loss: 4.4004\n",
      "Training Batch [300/313], Loss: 4.3999\n",
      "Training Batch [310/313], Loss: 4.3999\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3973\n",
      "Validation Batch [20/32], Loss: 4.3973\n",
      "Validation Batch [30/32], Loss: 4.3975\n",
      "Validation Loss: 4.3974, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [7/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.3994\n",
      "Training Batch [20/313], Loss: 4.3999\n",
      "Training Batch [30/313], Loss: 4.3993\n",
      "Training Batch [40/313], Loss: 4.4003\n",
      "Training Batch [50/313], Loss: 4.3993\n",
      "Training Batch [60/313], Loss: 4.3998\n",
      "Training Batch [70/313], Loss: 4.3997\n",
      "Training Batch [80/313], Loss: 4.4001\n",
      "Training Batch [90/313], Loss: 4.4003\n",
      "Training Batch [100/313], Loss: 4.3995\n",
      "Training Batch [110/313], Loss: 4.4000\n",
      "Training Batch [120/313], Loss: 4.4003\n",
      "Training Batch [130/313], Loss: 4.4002\n",
      "Training Batch [140/313], Loss: 4.3997\n",
      "Training Batch [150/313], Loss: 4.3998\n",
      "Training Batch [160/313], Loss: 4.3993\n",
      "Training Batch [170/313], Loss: 4.3995\n",
      "Training Batch [180/313], Loss: 4.4004\n",
      "Training Batch [190/313], Loss: 4.4001\n",
      "Training Batch [200/313], Loss: 4.3995\n",
      "Training Batch [210/313], Loss: 4.4006\n",
      "Training Batch [220/313], Loss: 4.3995\n",
      "Training Batch [230/313], Loss: 4.4000\n",
      "Training Batch [240/313], Loss: 4.4000\n",
      "Training Batch [250/313], Loss: 4.3998\n",
      "Training Batch [260/313], Loss: 4.3997\n",
      "Training Batch [270/313], Loss: 4.3997\n",
      "Training Batch [280/313], Loss: 4.3994\n",
      "Training Batch [290/313], Loss: 4.4005\n",
      "Training Batch [300/313], Loss: 4.4000\n",
      "Training Batch [310/313], Loss: 4.3996\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3974\n",
      "Validation Batch [20/32], Loss: 4.3975\n",
      "Validation Batch [30/32], Loss: 4.3975\n",
      "Validation Loss: 4.3975, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [8/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.3998\n",
      "Training Batch [20/313], Loss: 4.4000\n",
      "Training Batch [30/313], Loss: 4.3998\n",
      "Training Batch [40/313], Loss: 4.3997\n",
      "Training Batch [50/313], Loss: 4.3995\n",
      "Training Batch [60/313], Loss: 4.3999\n",
      "Training Batch [70/313], Loss: 4.3993\n",
      "Training Batch [80/313], Loss: 4.4000\n",
      "Training Batch [90/313], Loss: 4.4000\n",
      "Training Batch [100/313], Loss: 4.3998\n",
      "Training Batch [110/313], Loss: 4.3997\n",
      "Training Batch [120/313], Loss: 4.3996\n",
      "Training Batch [130/313], Loss: 4.3996\n",
      "Training Batch [140/313], Loss: 4.3992\n",
      "Training Batch [150/313], Loss: 4.4004\n",
      "Training Batch [160/313], Loss: 4.3998\n",
      "Training Batch [170/313], Loss: 4.4005\n",
      "Training Batch [180/313], Loss: 4.4003\n",
      "Training Batch [190/313], Loss: 4.3995\n",
      "Training Batch [200/313], Loss: 4.3999\n",
      "Training Batch [210/313], Loss: 4.3997\n",
      "Training Batch [220/313], Loss: 4.3994\n",
      "Training Batch [230/313], Loss: 4.3998\n",
      "Training Batch [240/313], Loss: 4.3999\n",
      "Training Batch [250/313], Loss: 4.4001\n",
      "Training Batch [260/313], Loss: 4.4001\n",
      "Training Batch [270/313], Loss: 4.3995\n",
      "Training Batch [280/313], Loss: 4.3999\n",
      "Training Batch [290/313], Loss: 4.3993\n",
      "Training Batch [300/313], Loss: 4.4001\n",
      "Training Batch [310/313], Loss: 4.3994\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3972\n",
      "Validation Batch [20/32], Loss: 4.3973\n",
      "Validation Batch [30/32], Loss: 4.3973\n",
      "Validation Loss: 4.3973, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [9/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.4001\n",
      "Training Batch [20/313], Loss: 4.3997\n",
      "Training Batch [30/313], Loss: 4.4000\n",
      "Training Batch [40/313], Loss: 4.3999\n",
      "Training Batch [50/313], Loss: 4.3995\n",
      "Training Batch [60/313], Loss: 4.3999\n",
      "Training Batch [70/313], Loss: 4.4000\n",
      "Training Batch [80/313], Loss: 4.3997\n",
      "Training Batch [90/313], Loss: 4.4002\n",
      "Training Batch [100/313], Loss: 4.3996\n",
      "Training Batch [110/313], Loss: 4.3998\n",
      "Training Batch [120/313], Loss: 4.3999\n",
      "Training Batch [130/313], Loss: 4.4000\n",
      "Training Batch [140/313], Loss: 4.3998\n",
      "Training Batch [150/313], Loss: 4.4000\n",
      "Training Batch [160/313], Loss: 4.3999\n",
      "Training Batch [170/313], Loss: 4.4002\n",
      "Training Batch [180/313], Loss: 4.4000\n",
      "Training Batch [190/313], Loss: 4.3994\n",
      "Training Batch [200/313], Loss: 4.3999\n",
      "Training Batch [210/313], Loss: 4.3999\n",
      "Training Batch [220/313], Loss: 4.3996\n",
      "Training Batch [230/313], Loss: 4.3995\n",
      "Training Batch [240/313], Loss: 4.4001\n",
      "Training Batch [250/313], Loss: 4.3997\n",
      "Training Batch [260/313], Loss: 4.3995\n",
      "Training Batch [270/313], Loss: 4.3997\n",
      "Training Batch [280/313], Loss: 4.3999\n",
      "Training Batch [290/313], Loss: 4.3998\n",
      "Training Batch [300/313], Loss: 4.3995\n",
      "Training Batch [310/313], Loss: 4.3997\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3972\n",
      "Validation Batch [20/32], Loss: 4.3972\n",
      "Validation Batch [30/32], Loss: 4.3973\n",
      "Validation Loss: 4.3973, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [10/10]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 4.4002\n",
      "Training Batch [20/313], Loss: 4.3992\n",
      "Training Batch [30/313], Loss: 4.3998\n",
      "Training Batch [40/313], Loss: 4.4001\n",
      "Training Batch [50/313], Loss: 4.4002\n",
      "Training Batch [60/313], Loss: 4.3996\n",
      "Training Batch [70/313], Loss: 4.4002\n",
      "Training Batch [80/313], Loss: 4.4000\n",
      "Training Batch [90/313], Loss: 4.3993\n",
      "Training Batch [100/313], Loss: 4.3997\n",
      "Training Batch [110/313], Loss: 4.4001\n",
      "Training Batch [120/313], Loss: 4.4000\n",
      "Training Batch [130/313], Loss: 4.3999\n",
      "Training Batch [140/313], Loss: 4.3996\n",
      "Training Batch [150/313], Loss: 4.3992\n",
      "Training Batch [160/313], Loss: 4.3995\n",
      "Training Batch [170/313], Loss: 4.4001\n",
      "Training Batch [180/313], Loss: 4.4000\n",
      "Training Batch [190/313], Loss: 4.3996\n",
      "Training Batch [200/313], Loss: 4.3999\n",
      "Training Batch [210/313], Loss: 4.3995\n",
      "Training Batch [220/313], Loss: 4.4001\n",
      "Training Batch [230/313], Loss: 4.4002\n",
      "Training Batch [240/313], Loss: 4.3998\n",
      "Training Batch [250/313], Loss: 4.3999\n",
      "Training Batch [260/313], Loss: 4.3996\n",
      "Training Batch [270/313], Loss: 4.3999\n",
      "Training Batch [280/313], Loss: 4.3996\n",
      "Training Batch [290/313], Loss: 4.3992\n",
      "Training Batch [300/313], Loss: 4.3996\n",
      "Training Batch [310/313], Loss: 4.3995\n",
      "Training Loss: 4.3998\n",
      "Validation Batch [10/32], Loss: 4.3969\n",
      "Validation Batch [20/32], Loss: 4.3970\n",
      "Validation Batch [30/32], Loss: 4.3972\n",
      "New best model saved with validation loss: 4.3970\n",
      "Validation Loss: 4.3970, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "trained_model, training_losses, validation_losses, training_accuracies, validation_accuracies = train_model(\n",
    "    model_MNV3, criterion, optimizer, train_loader, val_loader, num_epochs, early_stopper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d6a5e-b70d-43b8-b357-539e39f4f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "trained_model, training_losses, validation_losses, training_accuracies, validation_accuracies = train_model(\n",
    "    model_MNV3, criterion, optimizer, train_loader, val_loader, num_epochs, early_stopper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7ce3a340-a38d-462f-ba8b-57e89dbd86fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 120])\n",
      "Yaw Prediction: 50\n",
      "Pitch Prediction: 24\n",
      "Roll Prediction: 33\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pretrained MobileNetV3 model\n",
    "model_MNV3 = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "# Define constants\n",
    "HIDDEN_FEATURES = 1024\n",
    "OUTPUT_FEATURES = 1280\n",
    "NUM_CLASSES = 66  # Number of classes for each roll and pitch\n",
    "NUM_CLASSES_YAW = 120\n",
    "\n",
    "# Define the custom classifier with three heads\n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(CustomHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, HIDDEN_FEATURES)\n",
    "        self.fc2 = nn.Linear(HIDDEN_FEATURES, OUTPUT_FEATURES)\n",
    "        self.fc_yaw = nn.Linear(OUTPUT_FEATURES, NUM_CLASSES_YAW)\n",
    "        self.fc_pitch = nn.Linear(OUTPUT_FEATURES, NUM_CLASSES)\n",
    "        self.fc_roll = nn.Linear(OUTPUT_FEATURES, NUM_CLASSES)\n",
    "        self.hardswish = nn.Hardswish()\n",
    "        self.dropout = nn.Dropout(p=0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hardswish(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.hardswish(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        yaw = self.fc_yaw(x)\n",
    "        pitch = self.fc_pitch(x)\n",
    "        roll = self.fc_roll(x)\n",
    "        return yaw, pitch, roll\n",
    "\n",
    "# Modify the MobileNetV3 model to use the custom classifier\n",
    "class ModifiedMobileNetV3(nn.Module):\n",
    "    def __init__(self, num_classes_yaw, num_classes_pitch, num_classes_roll):\n",
    "        super(ModifiedMobileNetV3, self).__init__()\n",
    "        self.backbone = models.mobilenet_v3_small(pretrained=True).features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = models.mobilenet_v3_small(pretrained=True).classifier[0].in_features\n",
    "        self.classifier = CustomHead(in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        yaw, pitch, roll = self.classifier(x)\n",
    "        return yaw, pitch, roll\n",
    "\n",
    "# Instantiate the modified model\n",
    "model_MNV3 = ModifiedMobileNetV3(NUM_CLASSES_YAW, NUM_CLASSES, NUM_CLASSES)\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the GPU\n",
    "model_MNV3 = model_MNV3.to(device)\n",
    "\n",
    "# Define image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Assuming the model expects 224x224 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Adjust mean and std to match your training\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = preprocess(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Load the model\n",
    "def load_model(model_path):\n",
    "    model = ModifiedMobileNetV3(NUM_CLASSES_YAW,NUM_CLASSES,NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Inference on a single image\n",
    "def infer_single_image(model, image_tensor):\n",
    "    with torch.no_grad():\n",
    "        yaw_pred, pitch_pred, roll_pred = model(image_tensor)\n",
    "    return yaw_pred, pitch_pred, roll_pred\n",
    "\n",
    "# Paths\n",
    "model_path = 'best_model.pth'  # Path to the saved model\n",
    "image_path = \"D:/Users/Seif_Eldin_Sameh/Desktop/grad_project/300WLPA_2d/test/AFW_134212_1_0_-0.492_0.469_-0.150.jpg\"  # Path to the image to test\n",
    "\n",
    "# Load image\n",
    "image_tensor = load_image(image_path)\n",
    "\n",
    "# Load model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Perform inference\n",
    "yaw_pred, pitch_pred, roll_pred = infer_single_image(model, image_tensor)\n",
    "print(yaw_pred.shape)\n",
    "# Convert predictions to class labels\n",
    "yaw_pred_label = torch.argmax(yaw_pred, dim=1).item()\n",
    "pitch_pred_label = torch.argmax(pitch_pred, dim=1).item()\n",
    "roll_pred_label = torch.argmax(roll_pred, dim=1).item()\n",
    "\n",
    "# Print predictions\n",
    "print(f'Yaw Prediction: {yaw_pred_label}')\n",
    "print(f'Pitch Prediction: {pitch_pred_label}')\n",
    "print(f'Roll Prediction: {roll_pred_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec080f09-2744-4a6b-8843-0e04ec06e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9725\n",
      "Training Batch [20/313], Loss: 17.9732\n",
      "Training Batch [30/313], Loss: 17.9725\n",
      "Training Batch [40/313], Loss: 17.9733\n",
      "Training Batch [50/313], Loss: 17.9731\n",
      "Training Batch [60/313], Loss: 17.9728\n",
      "Training Batch [70/313], Loss: 17.9724\n",
      "Training Batch [80/313], Loss: 17.9727\n",
      "Training Batch [90/313], Loss: 17.9728\n",
      "Training Batch [100/313], Loss: 17.9732\n",
      "Training Batch [110/313], Loss: 17.9719\n",
      "Training Batch [120/313], Loss: 17.9726\n",
      "Training Batch [130/313], Loss: 17.9729\n",
      "Training Batch [140/313], Loss: 17.9734\n",
      "Training Batch [150/313], Loss: 17.9725\n",
      "Training Batch [160/313], Loss: 17.9729\n",
      "Training Batch [170/313], Loss: 17.9736\n",
      "Training Batch [180/313], Loss: 17.9716\n",
      "Training Batch [190/313], Loss: 17.9724\n",
      "Training Batch [200/313], Loss: 17.9723\n",
      "Training Batch [210/313], Loss: 17.9730\n",
      "Training Batch [220/313], Loss: 17.9727\n",
      "Training Batch [230/313], Loss: 17.9730\n",
      "Training Batch [240/313], Loss: 17.9724\n",
      "Training Batch [250/313], Loss: 17.9732\n",
      "Training Batch [260/313], Loss: 17.9731\n",
      "Training Batch [270/313], Loss: 17.9737\n",
      "Training Batch [280/313], Loss: 17.9733\n",
      "Training Batch [290/313], Loss: 17.9729\n",
      "Training Batch [300/313], Loss: 17.9722\n",
      "Training Batch [310/313], Loss: 17.9741\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9681\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "New best model saved with validation loss: 17.9682\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [2/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9727\n",
      "Training Batch [20/313], Loss: 17.9732\n",
      "Training Batch [30/313], Loss: 17.9739\n",
      "Training Batch [40/313], Loss: 17.9732\n",
      "Training Batch [50/313], Loss: 17.9728\n",
      "Training Batch [60/313], Loss: 17.9733\n",
      "Training Batch [70/313], Loss: 17.9729\n",
      "Training Batch [80/313], Loss: 17.9723\n",
      "Training Batch [90/313], Loss: 17.9728\n",
      "Training Batch [100/313], Loss: 17.9727\n",
      "Training Batch [110/313], Loss: 17.9734\n",
      "Training Batch [120/313], Loss: 17.9733\n",
      "Training Batch [130/313], Loss: 17.9735\n",
      "Training Batch [140/313], Loss: 17.9728\n",
      "Training Batch [150/313], Loss: 17.9726\n",
      "Training Batch [160/313], Loss: 17.9735\n",
      "Training Batch [170/313], Loss: 17.9735\n",
      "Training Batch [180/313], Loss: 17.9726\n",
      "Training Batch [190/313], Loss: 17.9732\n",
      "Training Batch [200/313], Loss: 17.9729\n",
      "Training Batch [210/313], Loss: 17.9733\n",
      "Training Batch [220/313], Loss: 17.9730\n",
      "Training Batch [230/313], Loss: 17.9730\n",
      "Training Batch [240/313], Loss: 17.9736\n",
      "Training Batch [250/313], Loss: 17.9736\n",
      "Training Batch [260/313], Loss: 17.9726\n",
      "Training Batch [270/313], Loss: 17.9727\n",
      "Training Batch [280/313], Loss: 17.9735\n",
      "Training Batch [290/313], Loss: 17.9728\n",
      "Training Batch [300/313], Loss: 17.9733\n",
      "Training Batch [310/313], Loss: 17.9728\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9683, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [3/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9731\n",
      "Training Batch [20/313], Loss: 17.9733\n",
      "Training Batch [30/313], Loss: 17.9727\n",
      "Training Batch [40/313], Loss: 17.9724\n",
      "Training Batch [50/313], Loss: 17.9729\n",
      "Training Batch [60/313], Loss: 17.9728\n",
      "Training Batch [70/313], Loss: 17.9739\n",
      "Training Batch [80/313], Loss: 17.9730\n",
      "Training Batch [90/313], Loss: 17.9731\n",
      "Training Batch [100/313], Loss: 17.9728\n",
      "Training Batch [110/313], Loss: 17.9742\n",
      "Training Batch [120/313], Loss: 17.9735\n",
      "Training Batch [130/313], Loss: 17.9731\n",
      "Training Batch [140/313], Loss: 17.9721\n",
      "Training Batch [150/313], Loss: 17.9727\n",
      "Training Batch [160/313], Loss: 17.9733\n",
      "Training Batch [170/313], Loss: 17.9729\n",
      "Training Batch [180/313], Loss: 17.9733\n",
      "Training Batch [190/313], Loss: 17.9731\n",
      "Training Batch [200/313], Loss: 17.9728\n",
      "Training Batch [210/313], Loss: 17.9736\n",
      "Training Batch [220/313], Loss: 17.9723\n",
      "Training Batch [230/313], Loss: 17.9719\n",
      "Training Batch [240/313], Loss: 17.9732\n",
      "Training Batch [250/313], Loss: 17.9735\n",
      "Training Batch [260/313], Loss: 17.9722\n",
      "Training Batch [270/313], Loss: 17.9732\n",
      "Training Batch [280/313], Loss: 17.9734\n",
      "Training Batch [290/313], Loss: 17.9735\n",
      "Training Batch [300/313], Loss: 17.9727\n",
      "Training Batch [310/313], Loss: 17.9730\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9683, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [4/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9726\n",
      "Training Batch [20/313], Loss: 17.9731\n",
      "Training Batch [30/313], Loss: 17.9720\n",
      "Training Batch [40/313], Loss: 17.9728\n",
      "Training Batch [50/313], Loss: 17.9726\n",
      "Training Batch [60/313], Loss: 17.9728\n",
      "Training Batch [70/313], Loss: 17.9720\n",
      "Training Batch [80/313], Loss: 17.9726\n",
      "Training Batch [90/313], Loss: 17.9728\n",
      "Training Batch [100/313], Loss: 17.9735\n",
      "Training Batch [110/313], Loss: 17.9732\n",
      "Training Batch [120/313], Loss: 17.9728\n",
      "Training Batch [130/313], Loss: 17.9725\n",
      "Training Batch [140/313], Loss: 17.9725\n",
      "Training Batch [150/313], Loss: 17.9729\n",
      "Training Batch [160/313], Loss: 17.9729\n",
      "Training Batch [170/313], Loss: 17.9727\n",
      "Training Batch [180/313], Loss: 17.9738\n",
      "Training Batch [190/313], Loss: 17.9729\n",
      "Training Batch [200/313], Loss: 17.9730\n",
      "Training Batch [210/313], Loss: 17.9724\n",
      "Training Batch [220/313], Loss: 17.9730\n",
      "Training Batch [230/313], Loss: 17.9739\n",
      "Training Batch [240/313], Loss: 17.9720\n",
      "Training Batch [250/313], Loss: 17.9726\n",
      "Training Batch [260/313], Loss: 17.9725\n",
      "Training Batch [270/313], Loss: 17.9722\n",
      "Training Batch [280/313], Loss: 17.9727\n",
      "Training Batch [290/313], Loss: 17.9732\n",
      "Training Batch [300/313], Loss: 17.9721\n",
      "Training Batch [310/313], Loss: 17.9732\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9683, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [5/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9728\n",
      "Training Batch [20/313], Loss: 17.9738\n",
      "Training Batch [30/313], Loss: 17.9731\n",
      "Training Batch [40/313], Loss: 17.9733\n",
      "Training Batch [50/313], Loss: 17.9714\n",
      "Training Batch [60/313], Loss: 17.9730\n",
      "Training Batch [70/313], Loss: 17.9722\n",
      "Training Batch [80/313], Loss: 17.9725\n",
      "Training Batch [90/313], Loss: 17.9732\n",
      "Training Batch [100/313], Loss: 17.9729\n",
      "Training Batch [110/313], Loss: 17.9723\n",
      "Training Batch [120/313], Loss: 17.9740\n",
      "Training Batch [130/313], Loss: 17.9722\n",
      "Training Batch [140/313], Loss: 17.9738\n",
      "Training Batch [150/313], Loss: 17.9717\n",
      "Training Batch [160/313], Loss: 17.9728\n",
      "Training Batch [170/313], Loss: 17.9732\n",
      "Training Batch [180/313], Loss: 17.9733\n",
      "Training Batch [190/313], Loss: 17.9736\n",
      "Training Batch [200/313], Loss: 17.9726\n",
      "Training Batch [210/313], Loss: 17.9739\n",
      "Training Batch [220/313], Loss: 17.9733\n",
      "Training Batch [230/313], Loss: 17.9737\n",
      "Training Batch [240/313], Loss: 17.9733\n",
      "Training Batch [250/313], Loss: 17.9730\n",
      "Training Batch [260/313], Loss: 17.9735\n",
      "Training Batch [270/313], Loss: 17.9730\n",
      "Training Batch [280/313], Loss: 17.9725\n",
      "Training Batch [290/313], Loss: 17.9729\n",
      "Training Batch [300/313], Loss: 17.9728\n",
      "Training Batch [310/313], Loss: 17.9724\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [6/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9727\n",
      "Training Batch [20/313], Loss: 17.9728\n",
      "Training Batch [30/313], Loss: 17.9732\n",
      "Training Batch [40/313], Loss: 17.9734\n",
      "Training Batch [50/313], Loss: 17.9735\n",
      "Training Batch [60/313], Loss: 17.9727\n",
      "Training Batch [70/313], Loss: 17.9730\n",
      "Training Batch [80/313], Loss: 17.9735\n",
      "Training Batch [90/313], Loss: 17.9732\n",
      "Training Batch [100/313], Loss: 17.9734\n",
      "Training Batch [110/313], Loss: 17.9725\n",
      "Training Batch [120/313], Loss: 17.9726\n",
      "Training Batch [130/313], Loss: 17.9727\n",
      "Training Batch [140/313], Loss: 17.9729\n",
      "Training Batch [150/313], Loss: 17.9728\n",
      "Training Batch [160/313], Loss: 17.9729\n",
      "Training Batch [170/313], Loss: 17.9729\n",
      "Training Batch [180/313], Loss: 17.9731\n",
      "Training Batch [190/313], Loss: 17.9724\n",
      "Training Batch [200/313], Loss: 17.9733\n",
      "Training Batch [210/313], Loss: 17.9731\n",
      "Training Batch [220/313], Loss: 17.9722\n",
      "Training Batch [230/313], Loss: 17.9724\n",
      "Training Batch [240/313], Loss: 17.9729\n",
      "Training Batch [250/313], Loss: 17.9728\n",
      "Training Batch [260/313], Loss: 17.9732\n",
      "Training Batch [270/313], Loss: 17.9734\n",
      "Training Batch [280/313], Loss: 17.9729\n",
      "Training Batch [290/313], Loss: 17.9727\n",
      "Training Batch [300/313], Loss: 17.9735\n",
      "Training Batch [310/313], Loss: 17.9731\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [7/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9725\n",
      "Training Batch [20/313], Loss: 17.9731\n",
      "Training Batch [30/313], Loss: 17.9729\n",
      "Training Batch [40/313], Loss: 17.9722\n",
      "Training Batch [50/313], Loss: 17.9724\n",
      "Training Batch [60/313], Loss: 17.9729\n",
      "Training Batch [70/313], Loss: 17.9724\n",
      "Training Batch [80/313], Loss: 17.9729\n",
      "Training Batch [90/313], Loss: 17.9726\n",
      "Training Batch [100/313], Loss: 17.9736\n",
      "Training Batch [110/313], Loss: 17.9726\n",
      "Training Batch [120/313], Loss: 17.9724\n",
      "Training Batch [130/313], Loss: 17.9728\n",
      "Training Batch [140/313], Loss: 17.9724\n",
      "Training Batch [150/313], Loss: 17.9723\n",
      "Training Batch [160/313], Loss: 17.9733\n",
      "Training Batch [170/313], Loss: 17.9727\n",
      "Training Batch [180/313], Loss: 17.9726\n",
      "Training Batch [190/313], Loss: 17.9736\n",
      "Training Batch [200/313], Loss: 17.9737\n",
      "Training Batch [210/313], Loss: 17.9724\n",
      "Training Batch [220/313], Loss: 17.9726\n",
      "Training Batch [230/313], Loss: 17.9729\n",
      "Training Batch [240/313], Loss: 17.9727\n",
      "Training Batch [250/313], Loss: 17.9733\n",
      "Training Batch [260/313], Loss: 17.9729\n",
      "Training Batch [270/313], Loss: 17.9730\n",
      "Training Batch [280/313], Loss: 17.9730\n",
      "Training Batch [290/313], Loss: 17.9732\n",
      "Training Batch [300/313], Loss: 17.9724\n",
      "Training Batch [310/313], Loss: 17.9731\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9681\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "New best model saved with validation loss: 17.9682\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [8/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9723\n",
      "Training Batch [20/313], Loss: 17.9725\n",
      "Training Batch [30/313], Loss: 17.9733\n",
      "Training Batch [40/313], Loss: 17.9725\n",
      "Training Batch [50/313], Loss: 17.9723\n",
      "Training Batch [60/313], Loss: 17.9729\n",
      "Training Batch [70/313], Loss: 17.9729\n",
      "Training Batch [80/313], Loss: 17.9735\n",
      "Training Batch [90/313], Loss: 17.9726\n",
      "Training Batch [100/313], Loss: 17.9734\n",
      "Training Batch [110/313], Loss: 17.9737\n",
      "Training Batch [120/313], Loss: 17.9741\n",
      "Training Batch [130/313], Loss: 17.9727\n",
      "Training Batch [140/313], Loss: 17.9721\n",
      "Training Batch [150/313], Loss: 17.9728\n",
      "Training Batch [160/313], Loss: 17.9728\n",
      "Training Batch [170/313], Loss: 17.9732\n",
      "Training Batch [180/313], Loss: 17.9731\n",
      "Training Batch [190/313], Loss: 17.9733\n",
      "Training Batch [200/313], Loss: 17.9725\n",
      "Training Batch [210/313], Loss: 17.9724\n",
      "Training Batch [220/313], Loss: 17.9731\n",
      "Training Batch [230/313], Loss: 17.9729\n",
      "Training Batch [240/313], Loss: 17.9732\n",
      "Training Batch [250/313], Loss: 17.9724\n",
      "Training Batch [260/313], Loss: 17.9727\n",
      "Training Batch [270/313], Loss: 17.9731\n",
      "Training Batch [280/313], Loss: 17.9723\n",
      "Training Batch [290/313], Loss: 17.9736\n",
      "Training Batch [300/313], Loss: 17.9718\n",
      "Training Batch [310/313], Loss: 17.9728\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [9/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9734\n",
      "Training Batch [20/313], Loss: 17.9733\n",
      "Training Batch [30/313], Loss: 17.9731\n",
      "Training Batch [40/313], Loss: 17.9734\n",
      "Training Batch [50/313], Loss: 17.9729\n",
      "Training Batch [60/313], Loss: 17.9731\n",
      "Training Batch [70/313], Loss: 17.9726\n",
      "Training Batch [80/313], Loss: 17.9732\n",
      "Training Batch [90/313], Loss: 17.9729\n",
      "Training Batch [100/313], Loss: 17.9721\n",
      "Training Batch [110/313], Loss: 17.9733\n",
      "Training Batch [120/313], Loss: 17.9722\n",
      "Training Batch [130/313], Loss: 17.9729\n",
      "Training Batch [140/313], Loss: 17.9726\n",
      "Training Batch [150/313], Loss: 17.9731\n",
      "Training Batch [160/313], Loss: 17.9729\n",
      "Training Batch [170/313], Loss: 17.9723\n",
      "Training Batch [180/313], Loss: 17.9732\n",
      "Training Batch [190/313], Loss: 17.9722\n",
      "Training Batch [200/313], Loss: 17.9727\n",
      "Training Batch [210/313], Loss: 17.9721\n",
      "Training Batch [220/313], Loss: 17.9741\n",
      "Training Batch [230/313], Loss: 17.9734\n",
      "Training Batch [240/313], Loss: 17.9727\n",
      "Training Batch [250/313], Loss: 17.9730\n",
      "Training Batch [260/313], Loss: 17.9733\n",
      "Training Batch [270/313], Loss: 17.9734\n",
      "Training Batch [280/313], Loss: 17.9737\n",
      "Training Batch [290/313], Loss: 17.9723\n",
      "Training Batch [300/313], Loss: 17.9726\n",
      "Training Batch [310/313], Loss: 17.9733\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [10/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9731\n",
      "Training Batch [20/313], Loss: 17.9724\n",
      "Training Batch [30/313], Loss: 17.9732\n",
      "Training Batch [40/313], Loss: 17.9735\n",
      "Training Batch [50/313], Loss: 17.9732\n",
      "Training Batch [60/313], Loss: 17.9736\n",
      "Training Batch [70/313], Loss: 17.9728\n",
      "Training Batch [80/313], Loss: 17.9733\n",
      "Training Batch [90/313], Loss: 17.9734\n",
      "Training Batch [100/313], Loss: 17.9727\n",
      "Training Batch [110/313], Loss: 17.9725\n",
      "Training Batch [120/313], Loss: 17.9726\n",
      "Training Batch [130/313], Loss: 17.9726\n",
      "Training Batch [140/313], Loss: 17.9717\n",
      "Training Batch [150/313], Loss: 17.9726\n",
      "Training Batch [160/313], Loss: 17.9733\n",
      "Training Batch [170/313], Loss: 17.9733\n",
      "Training Batch [180/313], Loss: 17.9739\n",
      "Training Batch [190/313], Loss: 17.9726\n",
      "Training Batch [200/313], Loss: 17.9738\n",
      "Training Batch [210/313], Loss: 17.9726\n",
      "Training Batch [220/313], Loss: 17.9729\n",
      "Training Batch [230/313], Loss: 17.9726\n",
      "Training Batch [240/313], Loss: 17.9728\n",
      "Training Batch [250/313], Loss: 17.9732\n",
      "Training Batch [260/313], Loss: 17.9730\n",
      "Training Batch [270/313], Loss: 17.9734\n",
      "Training Batch [280/313], Loss: 17.9733\n",
      "Training Batch [290/313], Loss: 17.9732\n",
      "Training Batch [300/313], Loss: 17.9728\n",
      "Training Batch [310/313], Loss: 17.9737\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [11/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9738\n",
      "Training Batch [20/313], Loss: 17.9739\n",
      "Training Batch [30/313], Loss: 17.9732\n",
      "Training Batch [40/313], Loss: 17.9730\n",
      "Training Batch [50/313], Loss: 17.9721\n",
      "Training Batch [60/313], Loss: 17.9732\n",
      "Training Batch [70/313], Loss: 17.9731\n",
      "Training Batch [80/313], Loss: 17.9737\n",
      "Training Batch [90/313], Loss: 17.9725\n",
      "Training Batch [100/313], Loss: 17.9727\n",
      "Training Batch [110/313], Loss: 17.9728\n",
      "Training Batch [120/313], Loss: 17.9726\n",
      "Training Batch [130/313], Loss: 17.9728\n",
      "Training Batch [140/313], Loss: 17.9721\n",
      "Training Batch [150/313], Loss: 17.9729\n",
      "Training Batch [160/313], Loss: 17.9729\n",
      "Training Batch [170/313], Loss: 17.9727\n",
      "Training Batch [180/313], Loss: 17.9734\n",
      "Training Batch [190/313], Loss: 17.9729\n",
      "Training Batch [200/313], Loss: 17.9723\n",
      "Training Batch [210/313], Loss: 17.9736\n",
      "Training Batch [220/313], Loss: 17.9740\n",
      "Training Batch [230/313], Loss: 17.9726\n",
      "Training Batch [240/313], Loss: 17.9730\n",
      "Training Batch [250/313], Loss: 17.9726\n",
      "Training Batch [260/313], Loss: 17.9725\n",
      "Training Batch [270/313], Loss: 17.9732\n",
      "Training Batch [280/313], Loss: 17.9734\n",
      "Training Batch [290/313], Loss: 17.9733\n",
      "Training Batch [300/313], Loss: 17.9726\n",
      "Training Batch [310/313], Loss: 17.9725\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9683, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [12/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9734\n",
      "Training Batch [20/313], Loss: 17.9734\n",
      "Training Batch [30/313], Loss: 17.9726\n",
      "Training Batch [40/313], Loss: 17.9727\n",
      "Training Batch [50/313], Loss: 17.9726\n",
      "Training Batch [60/313], Loss: 17.9730\n",
      "Training Batch [70/313], Loss: 17.9730\n",
      "Training Batch [80/313], Loss: 17.9733\n",
      "Training Batch [90/313], Loss: 17.9733\n",
      "Training Batch [100/313], Loss: 17.9726\n",
      "Training Batch [110/313], Loss: 17.9729\n",
      "Training Batch [120/313], Loss: 17.9727\n",
      "Training Batch [130/313], Loss: 17.9734\n",
      "Training Batch [140/313], Loss: 17.9733\n",
      "Training Batch [150/313], Loss: 17.9733\n",
      "Training Batch [160/313], Loss: 17.9723\n",
      "Training Batch [170/313], Loss: 17.9728\n",
      "Training Batch [180/313], Loss: 17.9738\n",
      "Training Batch [190/313], Loss: 17.9727\n",
      "Training Batch [200/313], Loss: 17.9721\n",
      "Training Batch [210/313], Loss: 17.9727\n",
      "Training Batch [220/313], Loss: 17.9729\n",
      "Training Batch [230/313], Loss: 17.9733\n",
      "Training Batch [240/313], Loss: 17.9740\n",
      "Training Batch [250/313], Loss: 17.9727\n",
      "Training Batch [260/313], Loss: 17.9723\n",
      "Training Batch [270/313], Loss: 17.9724\n",
      "Training Batch [280/313], Loss: 17.9729\n",
      "Training Batch [290/313], Loss: 17.9729\n",
      "Training Batch [300/313], Loss: 17.9732\n",
      "Training Batch [310/313], Loss: 17.9729\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [13/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9733\n",
      "Training Batch [20/313], Loss: 17.9729\n",
      "Training Batch [30/313], Loss: 17.9730\n",
      "Training Batch [40/313], Loss: 17.9730\n",
      "Training Batch [50/313], Loss: 17.9717\n",
      "Training Batch [60/313], Loss: 17.9731\n",
      "Training Batch [70/313], Loss: 17.9726\n",
      "Training Batch [80/313], Loss: 17.9725\n",
      "Training Batch [90/313], Loss: 17.9744\n",
      "Training Batch [100/313], Loss: 17.9730\n",
      "Training Batch [110/313], Loss: 17.9731\n",
      "Training Batch [120/313], Loss: 17.9726\n",
      "Training Batch [130/313], Loss: 17.9728\n",
      "Training Batch [140/313], Loss: 17.9728\n",
      "Training Batch [150/313], Loss: 17.9737\n",
      "Training Batch [160/313], Loss: 17.9733\n",
      "Training Batch [170/313], Loss: 17.9724\n",
      "Training Batch [180/313], Loss: 17.9730\n",
      "Training Batch [190/313], Loss: 17.9742\n",
      "Training Batch [200/313], Loss: 17.9732\n",
      "Training Batch [210/313], Loss: 17.9729\n",
      "Training Batch [220/313], Loss: 17.9734\n",
      "Training Batch [230/313], Loss: 17.9735\n",
      "Training Batch [240/313], Loss: 17.9723\n",
      "Training Batch [250/313], Loss: 17.9741\n",
      "Training Batch [260/313], Loss: 17.9723\n",
      "Training Batch [270/313], Loss: 17.9729\n",
      "Training Batch [280/313], Loss: 17.9733\n",
      "Training Batch [290/313], Loss: 17.9730\n",
      "Training Batch [300/313], Loss: 17.9728\n",
      "Training Batch [310/313], Loss: 17.9735\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9683, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [14/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9732\n",
      "Training Batch [20/313], Loss: 17.9725\n",
      "Training Batch [30/313], Loss: 17.9733\n",
      "Training Batch [40/313], Loss: 17.9731\n",
      "Training Batch [50/313], Loss: 17.9730\n",
      "Training Batch [60/313], Loss: 17.9734\n",
      "Training Batch [70/313], Loss: 17.9720\n",
      "Training Batch [80/313], Loss: 17.9729\n",
      "Training Batch [90/313], Loss: 17.9734\n",
      "Training Batch [100/313], Loss: 17.9725\n",
      "Training Batch [110/313], Loss: 17.9731\n",
      "Training Batch [120/313], Loss: 17.9736\n",
      "Training Batch [130/313], Loss: 17.9730\n",
      "Training Batch [140/313], Loss: 17.9724\n",
      "Training Batch [150/313], Loss: 17.9739\n",
      "Training Batch [160/313], Loss: 17.9731\n",
      "Training Batch [170/313], Loss: 17.9731\n",
      "Training Batch [180/313], Loss: 17.9739\n",
      "Training Batch [190/313], Loss: 17.9730\n",
      "Training Batch [200/313], Loss: 17.9732\n",
      "Training Batch [210/313], Loss: 17.9731\n",
      "Training Batch [220/313], Loss: 17.9723\n",
      "Training Batch [230/313], Loss: 17.9727\n",
      "Training Batch [240/313], Loss: 17.9730\n",
      "Training Batch [250/313], Loss: 17.9726\n",
      "Training Batch [260/313], Loss: 17.9726\n",
      "Training Batch [270/313], Loss: 17.9729\n",
      "Training Batch [280/313], Loss: 17.9729\n",
      "Training Batch [290/313], Loss: 17.9731\n",
      "Training Batch [300/313], Loss: 17.9731\n",
      "Training Batch [310/313], Loss: 17.9726\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [15/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9721\n",
      "Training Batch [20/313], Loss: 17.9737\n",
      "Training Batch [30/313], Loss: 17.9726\n",
      "Training Batch [40/313], Loss: 17.9735\n",
      "Training Batch [50/313], Loss: 17.9731\n",
      "Training Batch [60/313], Loss: 17.9734\n",
      "Training Batch [70/313], Loss: 17.9735\n",
      "Training Batch [80/313], Loss: 17.9731\n",
      "Training Batch [90/313], Loss: 17.9722\n",
      "Training Batch [100/313], Loss: 17.9721\n",
      "Training Batch [110/313], Loss: 17.9724\n",
      "Training Batch [120/313], Loss: 17.9720\n",
      "Training Batch [130/313], Loss: 17.9725\n",
      "Training Batch [140/313], Loss: 17.9728\n",
      "Training Batch [150/313], Loss: 17.9726\n",
      "Training Batch [160/313], Loss: 17.9731\n",
      "Training Batch [170/313], Loss: 17.9737\n",
      "Training Batch [180/313], Loss: 17.9730\n",
      "Training Batch [190/313], Loss: 17.9732\n",
      "Training Batch [200/313], Loss: 17.9734\n",
      "Training Batch [210/313], Loss: 17.9728\n",
      "Training Batch [220/313], Loss: 17.9730\n",
      "Training Batch [230/313], Loss: 17.9730\n",
      "Training Batch [240/313], Loss: 17.9731\n",
      "Training Batch [250/313], Loss: 17.9723\n",
      "Training Batch [260/313], Loss: 17.9728\n",
      "Training Batch [270/313], Loss: 17.9719\n",
      "Training Batch [280/313], Loss: 17.9723\n",
      "Training Batch [290/313], Loss: 17.9723\n",
      "Training Batch [300/313], Loss: 17.9732\n",
      "Training Batch [310/313], Loss: 17.9724\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9683, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [16/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9729\n",
      "Training Batch [20/313], Loss: 17.9733\n",
      "Training Batch [30/313], Loss: 17.9729\n",
      "Training Batch [40/313], Loss: 17.9727\n",
      "Training Batch [50/313], Loss: 17.9724\n",
      "Training Batch [60/313], Loss: 17.9731\n",
      "Training Batch [70/313], Loss: 17.9729\n",
      "Training Batch [80/313], Loss: 17.9730\n",
      "Training Batch [90/313], Loss: 17.9731\n",
      "Training Batch [100/313], Loss: 17.9736\n",
      "Training Batch [110/313], Loss: 17.9733\n",
      "Training Batch [120/313], Loss: 17.9733\n",
      "Training Batch [130/313], Loss: 17.9726\n",
      "Training Batch [140/313], Loss: 17.9724\n",
      "Training Batch [150/313], Loss: 17.9730\n",
      "Training Batch [160/313], Loss: 17.9733\n",
      "Training Batch [170/313], Loss: 17.9723\n",
      "Training Batch [180/313], Loss: 17.9725\n",
      "Training Batch [190/313], Loss: 17.9730\n",
      "Training Batch [200/313], Loss: 17.9729\n",
      "Training Batch [210/313], Loss: 17.9722\n",
      "Training Batch [220/313], Loss: 17.9725\n",
      "Training Batch [230/313], Loss: 17.9728\n",
      "Training Batch [240/313], Loss: 17.9727\n",
      "Training Batch [250/313], Loss: 17.9725\n",
      "Training Batch [260/313], Loss: 17.9730\n",
      "Training Batch [270/313], Loss: 17.9737\n",
      "Training Batch [280/313], Loss: 17.9733\n",
      "Training Batch [290/313], Loss: 17.9724\n",
      "Training Batch [300/313], Loss: 17.9731\n",
      "Training Batch [310/313], Loss: 17.9731\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [17/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9735\n",
      "Training Batch [20/313], Loss: 17.9732\n",
      "Training Batch [30/313], Loss: 17.9732\n",
      "Training Batch [40/313], Loss: 17.9729\n",
      "Training Batch [50/313], Loss: 17.9729\n",
      "Training Batch [60/313], Loss: 17.9727\n",
      "Training Batch [70/313], Loss: 17.9727\n",
      "Training Batch [80/313], Loss: 17.9731\n",
      "Training Batch [90/313], Loss: 17.9724\n",
      "Training Batch [100/313], Loss: 17.9726\n",
      "Training Batch [110/313], Loss: 17.9729\n",
      "Training Batch [120/313], Loss: 17.9732\n",
      "Training Batch [130/313], Loss: 17.9730\n",
      "Training Batch [140/313], Loss: 17.9727\n",
      "Training Batch [150/313], Loss: 17.9724\n",
      "Training Batch [160/313], Loss: 17.9721\n",
      "Training Batch [170/313], Loss: 17.9738\n",
      "Training Batch [180/313], Loss: 17.9730\n",
      "Training Batch [190/313], Loss: 17.9732\n",
      "Training Batch [200/313], Loss: 17.9724\n",
      "Training Batch [210/313], Loss: 17.9727\n",
      "Training Batch [220/313], Loss: 17.9728\n",
      "Training Batch [230/313], Loss: 17.9741\n",
      "Training Batch [240/313], Loss: 17.9733\n",
      "Training Batch [250/313], Loss: 17.9721\n",
      "Training Batch [260/313], Loss: 17.9724\n",
      "Training Batch [270/313], Loss: 17.9735\n",
      "Training Batch [280/313], Loss: 17.9733\n",
      "Training Batch [290/313], Loss: 17.9733\n",
      "Training Batch [300/313], Loss: 17.9726\n",
      "Training Batch [310/313], Loss: 17.9734\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [18/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9736\n",
      "Training Batch [20/313], Loss: 17.9734\n",
      "Training Batch [30/313], Loss: 17.9734\n",
      "Training Batch [40/313], Loss: 17.9725\n",
      "Training Batch [50/313], Loss: 17.9728\n",
      "Training Batch [60/313], Loss: 17.9726\n",
      "Training Batch [70/313], Loss: 17.9723\n",
      "Training Batch [80/313], Loss: 17.9743\n",
      "Training Batch [90/313], Loss: 17.9733\n",
      "Training Batch [100/313], Loss: 17.9731\n",
      "Training Batch [110/313], Loss: 17.9726\n",
      "Training Batch [120/313], Loss: 17.9731\n",
      "Training Batch [130/313], Loss: 17.9732\n",
      "Training Batch [140/313], Loss: 17.9733\n",
      "Training Batch [150/313], Loss: 17.9729\n",
      "Training Batch [160/313], Loss: 17.9728\n",
      "Training Batch [170/313], Loss: 17.9733\n",
      "Training Batch [180/313], Loss: 17.9723\n",
      "Training Batch [190/313], Loss: 17.9738\n",
      "Training Batch [200/313], Loss: 17.9731\n",
      "Training Batch [210/313], Loss: 17.9727\n",
      "Training Batch [220/313], Loss: 17.9736\n",
      "Training Batch [230/313], Loss: 17.9739\n",
      "Training Batch [240/313], Loss: 17.9726\n",
      "Training Batch [250/313], Loss: 17.9729\n",
      "Training Batch [260/313], Loss: 17.9723\n",
      "Training Batch [270/313], Loss: 17.9733\n",
      "Training Batch [280/313], Loss: 17.9724\n",
      "Training Batch [290/313], Loss: 17.9732\n",
      "Training Batch [300/313], Loss: 17.9731\n",
      "Training Batch [310/313], Loss: 17.9734\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9681\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [19/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9729\n",
      "Training Batch [20/313], Loss: 17.9729\n",
      "Training Batch [30/313], Loss: 17.9732\n",
      "Training Batch [40/313], Loss: 17.9728\n",
      "Training Batch [50/313], Loss: 17.9722\n",
      "Training Batch [60/313], Loss: 17.9725\n",
      "Training Batch [70/313], Loss: 17.9726\n",
      "Training Batch [80/313], Loss: 17.9721\n",
      "Training Batch [90/313], Loss: 17.9735\n",
      "Training Batch [100/313], Loss: 17.9719\n",
      "Training Batch [110/313], Loss: 17.9729\n",
      "Training Batch [120/313], Loss: 17.9723\n",
      "Training Batch [130/313], Loss: 17.9740\n",
      "Training Batch [140/313], Loss: 17.9728\n",
      "Training Batch [150/313], Loss: 17.9739\n",
      "Training Batch [160/313], Loss: 17.9734\n",
      "Training Batch [170/313], Loss: 17.9720\n",
      "Training Batch [180/313], Loss: 17.9734\n",
      "Training Batch [190/313], Loss: 17.9724\n",
      "Training Batch [200/313], Loss: 17.9731\n",
      "Training Batch [210/313], Loss: 17.9738\n",
      "Training Batch [220/313], Loss: 17.9718\n",
      "Training Batch [230/313], Loss: 17.9729\n",
      "Training Batch [240/313], Loss: 17.9736\n",
      "Training Batch [250/313], Loss: 17.9723\n",
      "Training Batch [260/313], Loss: 17.9726\n",
      "Training Batch [270/313], Loss: 17.9725\n",
      "Training Batch [280/313], Loss: 17.9727\n",
      "Training Batch [290/313], Loss: 17.9728\n",
      "Training Batch [300/313], Loss: 17.9722\n",
      "Training Batch [310/313], Loss: 17.9733\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9681\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "New best model saved with validation loss: 17.9682\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [20/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9726\n",
      "Training Batch [20/313], Loss: 17.9732\n",
      "Training Batch [30/313], Loss: 17.9730\n",
      "Training Batch [40/313], Loss: 17.9741\n",
      "Training Batch [50/313], Loss: 17.9743\n",
      "Training Batch [60/313], Loss: 17.9730\n",
      "Training Batch [70/313], Loss: 17.9723\n",
      "Training Batch [80/313], Loss: 17.9726\n",
      "Training Batch [90/313], Loss: 17.9729\n",
      "Training Batch [100/313], Loss: 17.9726\n",
      "Training Batch [110/313], Loss: 17.9730\n",
      "Training Batch [120/313], Loss: 17.9724\n",
      "Training Batch [130/313], Loss: 17.9734\n",
      "Training Batch [140/313], Loss: 17.9727\n",
      "Training Batch [150/313], Loss: 17.9738\n",
      "Training Batch [160/313], Loss: 17.9727\n",
      "Training Batch [170/313], Loss: 17.9730\n",
      "Training Batch [180/313], Loss: 17.9740\n",
      "Training Batch [190/313], Loss: 17.9737\n",
      "Training Batch [200/313], Loss: 17.9729\n",
      "Training Batch [210/313], Loss: 17.9728\n",
      "Training Batch [220/313], Loss: 17.9735\n",
      "Training Batch [230/313], Loss: 17.9738\n",
      "Training Batch [240/313], Loss: 17.9732\n",
      "Training Batch [250/313], Loss: 17.9727\n",
      "Training Batch [260/313], Loss: 17.9732\n",
      "Training Batch [270/313], Loss: 17.9720\n",
      "Training Batch [280/313], Loss: 17.9735\n",
      "Training Batch [290/313], Loss: 17.9730\n",
      "Training Batch [300/313], Loss: 17.9726\n",
      "Training Batch [310/313], Loss: 17.9731\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9681\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Epoch [21/100]\n",
      "--------------------------------------------------\n",
      "Training Batch [10/313], Loss: 17.9725\n",
      "Training Batch [20/313], Loss: 17.9722\n",
      "Training Batch [30/313], Loss: 17.9727\n",
      "Training Batch [40/313], Loss: 17.9715\n",
      "Training Batch [50/313], Loss: 17.9734\n",
      "Training Batch [60/313], Loss: 17.9726\n",
      "Training Batch [70/313], Loss: 17.9728\n",
      "Training Batch [80/313], Loss: 17.9735\n",
      "Training Batch [90/313], Loss: 17.9729\n",
      "Training Batch [100/313], Loss: 17.9728\n",
      "Training Batch [110/313], Loss: 17.9727\n",
      "Training Batch [120/313], Loss: 17.9728\n",
      "Training Batch [130/313], Loss: 17.9726\n",
      "Training Batch [140/313], Loss: 17.9727\n",
      "Training Batch [150/313], Loss: 17.9732\n",
      "Training Batch [160/313], Loss: 17.9741\n",
      "Training Batch [170/313], Loss: 17.9736\n",
      "Training Batch [180/313], Loss: 17.9730\n",
      "Training Batch [190/313], Loss: 17.9733\n",
      "Training Batch [200/313], Loss: 17.9732\n",
      "Training Batch [210/313], Loss: 17.9726\n",
      "Training Batch [220/313], Loss: 17.9729\n",
      "Training Batch [230/313], Loss: 17.9728\n",
      "Training Batch [240/313], Loss: 17.9732\n",
      "Training Batch [250/313], Loss: 17.9726\n",
      "Training Batch [260/313], Loss: 17.9733\n",
      "Training Batch [270/313], Loss: 17.9731\n",
      "Training Batch [280/313], Loss: 17.9734\n",
      "Training Batch [290/313], Loss: 17.9729\n",
      "Training Batch [300/313], Loss: 17.9720\n",
      "Training Batch [310/313], Loss: 17.9714\n",
      "Training Loss: 17.9730\n",
      "Validation Batch [10/32], Loss: 17.9680\n",
      "Validation Batch [20/32], Loss: 17.9682\n",
      "Validation Batch [30/32], Loss: 17.9686\n",
      "Validation Loss: 17.9682, Yaw Accuracy: 0.00%, Pitch Accuracy: 0.00%, Roll Accuracy: 0.00%\n",
      "Early stopping\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 66  # Number of classes for each roll and pitch\n",
    "NUM_CLASSES_YAW = 120\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = ModifiedMobileNetV3(NUM_CLASSES_YAW,NUM_CLASSES,NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "model_path = 'best_model.pth'  # Path to the saved model\n",
    "model = load_model(model_path)\n",
    "model = model.to(device)\n",
    "num_epochs = 100\n",
    "trained_model, training_losses, validation_losses, training_accuracies, validation_accuracies = train_model(\n",
    "    model, criterion, optimizer, train_loader, val_loader, num_epochs, early_stopper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7bcb7a-ceaf-4f13-8ad3-9af4266bb8c1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b38ac-12b6-445e-ac17-67dc43889584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.backends import cudnn\n",
    "from torchvision import transforms\n",
    "#import matplotlib\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "from Dataset import pose_eff_dataset,BIWI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e5602-e370-4918-8025-073303cefcce",
   "metadata": {},
   "source": [
    "# Setup Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a17fc7-b824-4c03-8531-f38532a1cfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae05e2-1717-4a22-8745-eddf68bc40ea",
   "metadata": {},
   "source": [
    "# Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2415cf4c-0dfd-4714-9d17-756b7e5f28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, alpha, beta):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.regression_loss = nn.MSELoss()\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, yaw_pred, pitch_pred, roll_pred, yaw_true, pitch_true, roll_true):\n",
    "        # Squeeze true labels if necessary (only if they are one-dimensional with singleton dimensions)\n",
    "        yaw_true = torch.squeeze(yaw_true, dim=1)\n",
    "        pitch_true = torch.squeeze(pitch_true, dim=1)\n",
    "        roll_true = torch.squeeze(roll_true, dim=1)\n",
    "\n",
    "        # Regression losses\n",
    "        reg_loss_yaw = self.regression_loss(yaw_pred.float(), yaw_true.float())\n",
    "        reg_loss_pitch = self.regression_loss(pitch_pred.float(), pitch_true.float())\n",
    "        reg_loss_roll = self.regression_loss(roll_pred.float(), roll_true.float())\n",
    "\n",
    "        # Classification losses (using CrossEntropyLoss)\n",
    "        cls_loss_yaw = self.classification_loss(yaw_pred, yaw_true)\n",
    "        cls_loss_pitch = self.classification_loss(pitch_pred, pitch_true)\n",
    "        cls_loss_roll = self.classification_loss(roll_pred, roll_true)\n",
    "\n",
    "        # Combined loss\n",
    "        reg_loss = (reg_loss_yaw*3 + reg_loss_pitch*2 + reg_loss_roll) / 6\n",
    "        cls_loss = (cls_loss_yaw*3 + cls_loss_pitch*2 + cls_loss_roll) / 6\n",
    "\n",
    "        loss = self.alpha * reg_loss + self.beta * cls_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffed0d5-9847-4389-a3d1-5c407a7079b3",
   "metadata": {},
   "source": [
    "# Adjust Environment and Save folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab616752-e1e3-49c2-b892-6cd4d7d03ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.enabled = True\n",
    "snapshot=''\n",
    "batch_size = 64\n",
    "gpu = 0\n",
    "b_scheduler = False\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4feb6-0c07-49c2-99a9-37b2d4067aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./output/snapshots'):\n",
    "        os.makedirs('./output/snapshots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ee858-4c72-4cb1-8231-9280144d2df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('output/snapshots/{}'.format(summary_name)):\n",
    "        os.makedirs('output/snapshots/{}'.format(summary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d73f5-150f-4f21-959c-604af79e859f",
   "metadata": {},
   "source": [
    "# Define Model Class and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194076c-8f3e-4bb7-8573-5cc1cb34d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained MobileNetV3 model\n",
    "model_MNV3 = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "# Define constants\n",
    "HIDDEN_FEATURES = 1024\n",
    "OUTPUT_FEATURES = 1280\n",
    "NUM_CLASSES = 66  # Number of classes for each roll and pitch\n",
    "NUM_CLASSES_YAW = 120\n",
    "\n",
    "# Define the custom classifier with three heads\n",
    "class CustomHead(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(CustomHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, HIDDEN_FEATURES)\n",
    "        self.bn1 = nn.BatchNorm1d(HIDDEN_FEATURES)\n",
    "        self.fc2 = nn.Linear(HIDDEN_FEATURES, OUTPUT_FEATURES)\n",
    "        self.bn2 = nn.BatchNorm1d(OUTPUT_FEATURES)\n",
    "        self.fc_yaw = nn.Linear(OUTPUT_FEATURES, NUM_CLASSES_YAW)\n",
    "        self.fc_pitch = nn.Linear(OUTPUT_FEATURES, NUM_CLASSES)\n",
    "        self.fc_roll = nn.Linear(OUTPUT_FEATURES, NUM_CLASSES)\n",
    "        self.hardswish = nn.Hardswish()\n",
    "        self.dropout = nn.Dropout(p=0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hardswish(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.hardswish(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        yaw = self.fc_yaw(x)\n",
    "        pitch = self.fc_pitch(x)\n",
    "        roll = self.fc_roll(x)\n",
    "        return yaw, pitch, roll\n",
    "\n",
    "# Modify the MobileNetV3 model to use the custom classifier\n",
    "class ModifiedMobileNetV3(nn.Module):\n",
    "    def __init__(self, num_classes_yaw, num_classes_pitch, num_classes_roll):\n",
    "        super(ModifiedMobileNetV3, self).__init__()\n",
    "        self.backbone = models.mobilenet_v3_small(pretrained=True).features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = models.mobilenet_v3_small(pretrained=True).classifier[0].in_features\n",
    "        self.classifier = CustomHead(in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        yaw, pitch, roll = self.classifier(x)\n",
    "        return yaw, pitch, roll\n",
    "\n",
    "# Instantiate the modified model\n",
    "model_MNV3 = ModifiedMobileNetV3(NUM_CLASSES_YAW, NUM_CLASSES, NUM_CLASSES)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model_MNV3 = model_MNV3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72ec1f-4d24-4f2c-9954-457c957ab781",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not snapshot == '':\n",
    "    saved_state_dict = torch.load(snapshot)\n",
    "    model.load_state_dict(saved_state_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce5b90-593c-42dd-9c10-2edc8b048562",
   "metadata": {},
   "source": [
    "# Load from Datasets Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f57a6-4f5e-49c9-8b89-7760425f1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data.')\n",
    "pkla=pd.read_pickle(\"./Datasets/300W_LP/300W_LP/file.pkl\")\n",
    "pkla = pkla.sample(frac=1, random_state=42)\n",
    "# train=pkla[:int(0.9*len(df_shuffled))]\n",
    "# test=pkla[int(0.9*len(df_shuffled)):]\n",
    "# test.reset_index(inplace=True)\n",
    "# print(test.head())\n",
    "normalize = transforms.Normalize(\n",
    "mean=[0.485, 0.456, 0.406],\n",
    "std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transformations = transforms.Compose([transforms.RandomResizedCrop(size=224,scale=(0.8,1)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    normalize])\n",
    "train_pose_dataset =pose_eff_dataset('./Datasets/300W_LP/300W_LP',\n",
    "                                pkla,\n",
    "                                transformations)\n",
    "# test_pose_dataset =pose_eff_dataset('./Datasets/300W_LP/300W_LP',\n",
    "#                                 test,\n",
    "#                                 transformations)\n",
    "test_pose_dataset =BIWI(\"E:/HeadPose/Training/Datasets/BIWI_done.npz\",\n",
    "                        transform=transformations,\n",
    "                        train_mode=False) \n",
    "train_effloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_pose_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4)\n",
    "\n",
    "test_effloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_pose_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee58fa-66de-4e29-9569-4e9742d30ed0",
   "metadata": {},
   "source": [
    "# Define Criterion and Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c827411-a2ff-4823-bf4d-949555084c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the custom loss with appropriate weights\n",
    "criterion = CustomLoss(alpha=1.0, beta=2.0)  # Adjust alpha and beta as needed\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(model_MNV3.classifier.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d31ba3-2500-4e27-87dd-10a28b7616d0",
   "metadata": {},
   "source": [
    "# Define EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100d80d-a8e2-449a-a813-36373209ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def early_stop(self, val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            return False\n",
    "\n",
    "        if val_loss < self.best_score - self.min_delta:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6faf07-3cad-4c6d-941b-1f5e62212a27",
   "metadata": {},
   "source": [
    "# Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57ee18-362a-4ac0-b0e8-a22bbd4cac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10, early_stopper=None, save_path='best_model.pth'):\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_val_loss = float('inf')  # Initialize with infinity\n",
    "    lowest_epoch_crossval = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "        print('-' * 50)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        processed_samples_train = 0\n",
    "        total_samples_train = len(train_loader.dataset)\n",
    "\n",
    "        for batch_idx, (inputs, labels) in tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "            inputs = inputs.to(device)\n",
    "            yaw_true = labels['yaw'].to(device)\n",
    "            pitch_true = labels['pitch'].to(device)\n",
    "            roll_true = labels['roll'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            yaw_pred, pitch_pred, roll_pred = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(yaw_pred, pitch_pred, roll_pred, yaw_true, pitch_true, roll_true)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            processed_samples_train += inputs.size(0)\n",
    "\n",
    "            # Print progress\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Training Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        epoch_loss = running_loss / total_samples_train\n",
    "        training_losses.append(epoch_loss)\n",
    "        print(f'Training Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        yaw_error = pitch_error = roll_error = 0.0\n",
    "        total_samples_val = len(val_loader.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Validation Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "                inputs = inputs.to(device)\n",
    "                yaw_true = labels['yaw'].to(device)\n",
    "                pitch_true = labels['pitch'].to(device)\n",
    "                roll_true = labels['roll'].to(device)\n",
    "        \n",
    "                yaw_pred, pitch_pred, roll_pred = model(inputs)\n",
    "\n",
    "                # Calculate MAE for yaw, pitch, and roll\n",
    "                yaw_error += torch.sum(torch.abs(yaw_true - yaw_pred))\n",
    "                pitch_error += torch.sum(torch.abs(pitch_true - pitch_pred))\n",
    "                roll_error += torch.sum(torch.abs(roll_true - roll_pred))\n",
    "        \n",
    "                # Compute loss\n",
    "                loss = criterion(yaw_pred, pitch_pred, roll_pred, yaw_true, pitch_true, roll_true)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Print progress\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f'Validation Batch [{batch_idx + 1}/{len(val_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            val_loss = val_running_loss / total_samples_val\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            yaw_mae = yaw_error / total_samples_val\n",
    "            pitch_mae = pitch_error / total_samples_val\n",
    "            roll_mae = roll_error / total_samples_val\n",
    "            mae = (yaw_mae + pitch_mae + roll_mae) / 3\n",
    "\n",
    "            print(f'Validation Loss: {val_loss:.4f}, MAE: {mae:.4f}, '\n",
    "                  f'Yaw MAE: {yaw_mae:.4f}, Pitch MAE: {pitch_mae:.4f}, Roll MAE: {roll_mae:.4f}')\n",
    "\n",
    "            # Save model with the lowest validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(best_model, save_path)\n",
    "                print(f'New best model saved with validation loss: {val_loss:.4f}')\n",
    "\n",
    "            # Update the best model based on MAE\n",
    "            if mae < lowest_epoch_crossval:\n",
    "                lowest_epoch_crossval = mae\n",
    "                lowest_yaw = yaw_mae\n",
    "                lowest_pitch = pitch_mae\n",
    "                lowest_roll = roll_mae\n",
    "                lowest_epoch_no = epoch\n",
    "\n",
    "        # Check early stopping condition\n",
    "        if early_stopper is not None and early_stopper.early_stop(val_loss):\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print('Training complete')\n",
    "    print(f\"Best MAE: {lowest_epoch_crossval:.4f}, Yaw MAE: {lowest_yaw:.4f}, Pitch MAE: {lowest_pitch:.4f}, Roll MAE: {lowest_roll:.4f}, Epoch: {lowest_epoch_no}\")\n",
    "\n",
    "    return model, training_losses, validation_losses, lowest_epoch_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36026815-f9e5-43e4-9578-49ddd0c48d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopper(patience=20, min_delta=0.5)\n",
    "trained_model, training_losses, validation_losses, training_accuracies, validation_accuracies = train_model(\n",
    "    model_MNV3, criterion, optimizer, train_effloader, test_effloader, num_epochs=100, early_stopper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
